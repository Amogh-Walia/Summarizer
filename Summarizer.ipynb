{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amogh-Walia/Summarizer/blob/main/Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YevTGlm6ngJc"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "from nltk.tag import pos_tag # for proper noun\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otfYuT0BngJj"
      },
      "outputs": [],
      "source": [
        "filename=\"C:\\\\summarization_dataset\\\\news_articles\\\\001.txt\"\n",
        "f = open((filename), \"r\")\n",
        "text=f.read() #append each line in the file to a list\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbAqkJzfngJj",
        "outputId": "a826d35f-2625-41f0-9427-95a0163e4fa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "326\n"
          ]
        }
      ],
      "source": [
        "sent_tokens = nltk.sent_tokenize(text)\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "word_tokens_lower=[word.lower() for word in word_tokens]\n",
        "stopWords = list(set(stopwords.words(\"english\")))\n",
        "word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n",
        "print(len(word_tokens_refined))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiJd0zWfngJl",
        "outputId": "15f7f3ed-8edb-4c3a-d0ff-fdad8bd00e88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])\n"
          ]
        }
      ],
      "source": [
        "# ......................feature 1 (cue phrases).................\n",
        "QPhrases=[\"incidentally\", \"example\", \"anyway\", \"furthermore\",\"according\"\n",
        "            \"first\", \"second\", \"then\", \"now\", \"thus\", \"moreover\", \"therefore\", \"hence\", \"lastly\", \"finally\", \"summary\"]\n",
        "\n",
        "cue_phrases={}\n",
        "for sentence in sent_tokens:\n",
        "    cue_phrases[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for word in word_tokens:\n",
        "        if word.lower() in QPhrases:\n",
        "            cue_phrases[sentence] += 1\n",
        "maximum_frequency = max(cue_phrases.values())\n",
        "for k in cue_phrases.keys():\n",
        "    try:\n",
        "        cue_phrases[k] = cue_phrases[k] / maximum_frequency\n",
        "        cue_phrases[k]=round(cue_phrases[k],3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "print(cue_phrases.values())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNqxShEwngJm",
        "outputId": "69d54ee4-5a06-4943-cc03-0418c42c3cf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Time Warner said on Friday that it now owns 8% of search-engine Google.']\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "print([sent_tokens[4]])\n",
        "print(cue_phrases[sent_tokens[4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XJSW_bGngJm",
        "outputId": "2f8fdedf-e18a-40b5-ed40-d766af960c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n"
          ]
        }
      ],
      "source": [
        "# .......................feature 2 (numerical data)...................\n",
        "numeric_data={}\n",
        "for sentence in sent_tokens:\n",
        "    numeric_data[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for word in word_tokens:\n",
        "        if word.isdigit():\n",
        "            numeric_data[sentence] += 1\n",
        "maximum_frequency = max(numeric_data.values())\n",
        "for k in numeric_data.keys():\n",
        "    try:\n",
        "        numeric_data[k] = (numeric_data[k]/maximum_frequency)\n",
        "        numeric_data[k] = round(numeric_data[k], 3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "print(numeric_data.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G5cSgJxngJn",
        "outputId": "df22788c-ada2-461e-e972-70a6a7f7bd01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([0.3, 0.7, 1, 0.9, 1, 1, 1, 0.8, 0.55, 0.6, 1, -0.15, 0.55, 0.5, 0.8, 0.95, 0.85, 0.65, 0.35, 0.9])\n"
          ]
        }
      ],
      "source": [
        "#....................feature -3 (sentence length)........................\n",
        "sent_len_score={}\n",
        "for sentence in sent_tokens:\n",
        "    sent_len_score[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    if len(word_tokens) in range(0,10):\n",
        "        sent_len_score[sentence]=1-0.05*(10-len(word_tokens))\n",
        "    elif len(word_tokens) in range(7,20):\n",
        "        sent_len_score[sentence]=1\n",
        "    else:\n",
        "        sent_len_score[sentence]=1-(0.05)*(len(word_tokens)-20)\n",
        "for k in sent_len_score.keys():\n",
        "    sent_len_score[k]=round(sent_len_score[k],4)\n",
        "print(sent_len_score.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "p98szv1ZngJo",
        "outputId": "50a1dbb9-e5c7-42cb-96a6-6e25add63888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.1, 0.1, 0.111, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n"
          ]
        }
      ],
      "source": [
        "#....................feature-4(sentence position)........................\n",
        "sentence_position={}\n",
        "d=1\n",
        "no_of_sent=len(sent_tokens)\n",
        "for i in range(no_of_sent):\n",
        "    a=1/d\n",
        "    b=1/(no_of_sent-d+1)\n",
        "    sentence_position[sent_tokens[d-1]]=max(a,b)\n",
        "    d=d+1\n",
        "for k in sentence_position.keys():\n",
        "    sentence_position[k]=round(sentence_position[k],3)\n",
        "print(sentence_position.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2Sh19ecngJp",
        "outputId": "1f14f55d-fb41-4702-a6f6-1c9acce68c9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([0.5, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.5])\n"
          ]
        }
      ],
      "source": [
        "#........................feature-5 (upper cases).................................\n",
        "upper_case={}\n",
        "for sentence in sent_tokens:\n",
        "    upper_case[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for k in word_tokens:\n",
        "        if k.isupper():\n",
        "            upper_case[sentence] += 1\n",
        "maximum_frequency = max(upper_case.values())\n",
        "for k in upper_case.keys():\n",
        "    try:\n",
        "        upper_case[k] = (upper_case[k]/maximum_frequency)\n",
        "        upper_case[k] = round(upper_case[k], 3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "print(upper_case.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpXSg87GngJr",
        "outputId": "0b5cabbd-01ed-4fd1-c023-fd14580f1a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([1.0, 0.167, 0.167, 0.5, 0.667, 0.167, 0.0, 0.167, 0.333, 0.833, 0.333, 0.667, 0.333, 0.333, 0.167, 0.5, 0.167, 0.0, 0.5, 0.333])\n"
          ]
        }
      ],
      "source": [
        "#......................... feature-6 (number of proper noun)...................\n",
        "proper_noun={}\n",
        "for sentence in sent_tokens:\n",
        "    tagged_sent = pos_tag(sentence.split())\n",
        "    propernouns = [word for word, pos in tagged_sent if pos == 'NNP']\n",
        "    proper_noun[sentence]=len(propernouns)\n",
        "maximum_frequency = max(proper_noun.values())\n",
        "for k in proper_noun.keys():\n",
        "    try:\n",
        "        proper_noun[k] = (proper_noun[k]/maximum_frequency)\n",
        "        proper_noun[k] = round(proper_noun[k], 3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "print(proper_noun.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpZRKXH_ngJs",
        "outputId": "9393f6c3-1a93-4150-85f5-329c52c9012b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([3.3, 2.367, 2.0, 2.15, 3.367, 1.834, 1.143, 2.092, 1.494, 3.5330000000000004, 1.433, 1.1280000000000001, 2.008, 0.976, 2.134, 2.65, 1.7670000000000001, 0.9830000000000001, 1.85, 3.733])\n"
          ]
        }
      ],
      "source": [
        "total_score={}\n",
        "for k in cue_phrases.keys():\n",
        "    total_score[k]=cue_phrases[k]+numeric_data[k]+sent_len_score[k]+sentence_position[k]+upper_case[k]+proper_noun[k]\n",
        "print(total_score.values())    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaKMx5V4ngJs",
        "outputId": "87ea6793-f191-4a09-971a-10528733b60d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier. Time Warner said on Friday that it now owns 8% of search-engine Google. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding. TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n"
          ]
        }
      ],
      "source": [
        "sumValues = 0\n",
        "for sentence in total_score: \n",
        "    sumValues += total_score[sentence] \n",
        "average = int(sumValues / len(total_score)) \n",
        "   \n",
        "# Storing sentences into our summary. \n",
        "summary = '' \n",
        "for sentence in sent_tokens: \n",
        "    if (sentence in total_score) and (total_score[sentence] > (1.3*average)): \n",
        "        summary += \" \" + sentence \n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBvM-K9OngJt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Summarizer.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}